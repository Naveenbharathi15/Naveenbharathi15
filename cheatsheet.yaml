# Port forwarding Kubernetes
kubectl port-forward piperr-web-6f54547758-6vfhz 8080:8080

# getting inside pod
kubectl exec -i -t piperr-scheduler-76d7c86bd9-pvhtl --container piperr-scheduler -- /bin/bash
kubectl exec -i -t piperr-reports-5bb8b9dfbf-f46gr -- /bin/bash

# Connecting to VM through SSH
1. Create SSH RSA Asymmetric Public/Private Key
---- ssh-keygen -t rsa -f ~/.ssh/[KEY_FILENAME] -C [USERNAME]
2. Connect to Compute Engine Instance
---- ssh -i PATH_TO_PRIVATE_KEY USERNAME@EXTERNAL_IP
ssh-i ~/h root0002@35.202.183.231
3. open the specified file in code editor
# gives read only permission 
chmod 400 crayon-aws.pem 
# to access vm
ssh ubuntu@44.242.98.62    and then enter password
# access vm and copy file from local to vm
scp -i Downloads/crayon-aws.pem Downloads/re_jira.zip ubuntu@44.242.98.62:/home/ubuntu/RE_JIRA_CONNECTOR
scp -i Downloads/royalenfield.pem /home/root0002/re_jira_connector/login/app.py ssh
:/home/ubuntu/re_jira_connector/login
# vm to local
scp -i /home/root0002/Downloads/royalenfield.pem ubuntu@54.183.123.115:/home/ubuntu/re_jira_integration.zip /home/root0002/RE/
# copy folder from local to vm
scp -r /home/root0002/vm/dags root0002@35.223.203.218:/home/root0002/airflow/
#  after creating vm you have to create firewall rule to expose the particular vp
go to create filrewall rule
# signin as root user
sudo su - to go in as root user

# docker socket
sudo chmod +777 /var/run/docker.sock

# docker run in specific port 
docker run -p {target_port}:{container_port} {image_name}

# accessing container from localhost
https://forums.docker.com/t/using-localhost-for-to-access-running-container/3148

# getting ip of a container
docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container-ID>

# deleting port
sudo lsof -i:{nameofport}
sudo kill {pid}

# copies the files fron the container to local file system
docker cp <containerId>:/file/path/within/container /host/path/target

# list of users gcloud
gcloud auth list

#GIT
git clone 'url'
# to switch branch 
git switch 
git branch 
git diff
git status 
git add
git commit -m "Messages"
git push
# authenticating git 
git push https://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@github.com/Naveenbharathi15/Naveenbharathi15.git

# To find and replace in all files 
ctrl + shift + f
ctrl + shift + r

# Merging DAGS
1. Check libraries
2. Merge variables
3. Change to  caps and check for all iknstances
4. Check for DAG name and start date in defined DAGS
5. Import tasks om next DAGS
6. Update Variables
7. Change Email content 
8. Update Dependency

# Git sync access only possible with kubernetes executor

# to display 100 rows in csv file
head -100 filename.csv > cat.csv

# Running a shell script
1. Create the sh file
2. copy and paste the commands with echo on front
3. check the file through cat command
4. run this ./filename.sh

# Filtering DataFrame Pandas
    # 1.
    stocks_df = pd.DataFrame({
        'Stock': ["Tesla","Moderna Inc","Facebook","Boeing"],
        'Price': [835,112,267,209],
        'Sector':["Technology","Health Technology","Technology","Aircraft"]})
    # 2.
    df1 = df[df['PartNumber']== j & docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' <container-ID>

    # 3. to filter through list
    df = df[df['ModelID'].isin(['VDLS46BB', '18t26UCSGmQf4AU5EdMXJw]).dt.date
    x = dt.datetime.strptime(mfg_date, '%Y-%m-%d').date()

# imports airflow 1.10.10
import time
import os
import re
from datetime import date, datetime, timedelta
import pandas as pd
from airflow import DAG
from airflow.contrib.hooks.wasb_hook import WasbHook
from airflow.contrib.operators.file_to_gcs import FileToGoogleCloudStorageOperator
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.contrib.operators import gcs_to_bq
from airflow.operators.python_operator import PythonOperator---
from airflow.operators.email_operator import EmailOperator
from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook
from airflow.utils.dates import days_ago
import smtplib
import mimetypes
import email
import email.mime.application
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication

# Connection GCP / blob
part_list_csv = f'part.csv'
storage_client = storage.Client()
bucket = storage_client.bucket(GCS_BUCKET)
print(storage.Blob(bucket=bucket, name=sales_csv).exists(storage_client))
print(storage.Blob(bucket=bucket, name=part_list_csv).exists(storage_client))
blob1 = bucket.blob(sales_csv)
blob1.download_to_filename("sales.csv")

# Airflow 
# to check default values
airflow info
# log information
airflow config list
# Backfill Command 
airflow backfill FAILURE_PREDICTION_MODEL_DAG_1 -m -s 2022-02-07 -e 2022-06-01"
# Trigger Command 
airflow trigger_dag DMS_SALES_DATA_INGESTION_PIPELINE

uselegacy = false



# Check for a file in azure blobpsql -h 34.68.215.72 -p 6000 -d postgres -U postgres
LOCAL_FILE = f'dms-{today}.csv'
AZURE_BLOB = f'poc-data/DMS/Delta-Data/dms-{today}.csv'
AZURE_CONTAINER = "175-warranty-analytics"
def check_blob():
    az_hook = WasbHook(wasb_conn_id='wasb_default')
    return az_hook.check_for_blob(container_name=AZURE_CONTAINER, blob_name=AZURE_BLOB)

# getting file from a azure blob
az_hook = WasbHook(wasb_conn_id='wasb_default')
az_hook.get_file(LOCAL_FILE, container_name=AZURE_CONTAINER, blob_name=AZURE_BLOB)

# Deleting a file in local
def delete_local_files():
  os.remove(LOCAL_FILE)

# For doubt in pandas refer transformation method from (dms JC pipeline)

# Operators in Airflow

gcs = LocalFilesystemToGCSOperator(
    task_id="local_to_gcs",
    src=LOCAL_FILE,
    dst=GCS_FILE_PATH,
    bucket=GCS_BUCKET,
    mime_type='text/csv',
    google_cloud_storage_conn_id=GCP_CONN_ID,
    dag=dag
)

delete = PythonOperator(
    task_id='delete_local_file',
    python_callable=delete_local_files,
    dag=dag
)

gcs_bq = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(
    task_id='gcs_to_bigquery',
    bucket=GCS_BUCKET,
    source_objects=[GCS_FILE_PATH],
    schema_fields=[
        {"name": "JobCard", "type": "STRING", "mode": "NULLABLE"},
        {"name": "PartNumber", "type": "STRING", "mode": "NULLABLE"},
        {"name": "LineAmount", "type": "FLOAT", "mode": "NULLABLE"},
        {"name": "PartDescription", "type": "STRING", "mode": "NULLABLE"},
        {"name": "ItemType", "type": "STRING", "mode": "NULLABLE"}, 
    ],
    destination_project_dataset_table=f'{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}',
    write_disposition='WRITE_APPEND',
    skip_leading_rows=1,
    allow_quoted_newlines=True,
    google_cloud_storage_conn_id=GCP_CONN_ID,
    bigquery_conn_id=GCP_CONN_ID,
    dag=dag
)

email_success = EmailOperator(
        task_id='send_email_on_success',
        to=["rakeshaswath@saturam.com","naveen@saturam.com"],
        subject='DMS JobCard data ingestion pipeline Success',
        html_content=""" <h3>DMS JobCard data ingestion pipeline ingested data successfully</h3> """,
        dag=dag
)

relation_task = BigQueryOperator(
    task_id='relation_tree',
    use_legacy_sql=False,
    sql=sql,
    destination_dataset_table=f'{GCP_PROJECT}:{BQ_DATASET}.{BQ_DESTINATION_TABLE}',
    write_disposition='WRITE_TRUNCATE',
    bigquery_conn_id=GCP_CONN_ID,
    dag=dag,
)

bo = BranchPythonOperator(
    task_id='choose_tasks',
    python_callable=choose_tasks,
    do_xcom_push=False,
    dag=dag
)

# converting from test to production dags
1. change destination table file path
2. DAG Date
3. Table Names
4. Send email RE(comment out)

# bash operator shell script
rm /usr/local/airflow/dags/*.csv || echo "hello"
rm rm /usr/local/airflow/dags/*.xls || echo "hello

# variable airflow json
{
"dms_jc_file_status": "false",
"dms_sales_file_status": "false",
"dms_call_centre_file_status": "false",
"dms_rsa_status": "false",
"ppap_file_status": "false",
"rsa_file_status": "false",
"social_media_file_status": "false",
"sap_vehicle_prod_file_status": "false",
"sap_concession_file_status" : "false"
}

# PSQL POSTGRES
    https://medium.com/coding-blocks/creating-user-database-and-adding-access-on-postgresql-8bfcd2f4a91e
    CREATE DATABASE naveensdb
    \c naveendb
    sudo -u postgres psql 
    psql -h <REMOTE HOST> -p <REMOTE PORT> -U <DB_USER> <DB_NAME>
    psql -h 34.68.215.72 -p 5432 -U superset superset
    CREATE DATABASE superset;
    CREATE USER superset WITH PASSWORD 'superset';
    GRANT ALL PRIVILEGES ON DATABASE "superset" to superset;
    #  to point schema in airflow
    GRANT ALL ON SCHEMA public TO airflow;
    psql -h 34.68.215.72 -p 6000 -d postgres -U postgres -W
    psql -h 34.68.215.72 -U superset -p 5432 -d superset < /home/root0002/Downloads/superset_bckup.sql
    # COPY
    psql -h 34.68.215.72 -U superset -d superset -c "\copy re_up FROM '/home/root0002/Downloads/upvpp.csv' with (format csv,header true, delimiter ',');"

    # backup file command in sql
    docker exec -u airflow 741b11dfd3e9 pg_dump -Fc airflow > db.sql
    # For Postgres
    pg_dump -U tecmint -h 10.10.20.10 -p 5432 tecmintdb > tecmintdb.sql
    # medium article for backup from any datasource
    https://medium.com/yavar/postgresql-using-copy-pg-dump-and-restore-b925f04c324d
    # backup fb in swpcific format
    pg_dump -Fc -U postgres -h 35.238.177.160 -p 5432 -d airflow-v2 > airflow-v2.db
    # migration from one db to other
    pg_dump -C -h localhost -U localuser dbname | psql -h remotehost -U remoteuser dbname


    # postgres conn url
    SQLALCHEMY_DATABASE_URI = 'postgresql://superset:superset@34.68.215.72/superset'


    # COPY RIGHT IN THE TERMINAL
    psql -h alkem.postgres.database.azure.com -U alkem@alkem -d postgres -c "\copy upvpp FROM '/home/root0002/Downloads/upvpp.csv' with (format csv,header true, delimiter ',');"
    COPY upvpp_v1 FROM ‘/home/root0002/Downloads/upvpp.csv’ DELIMETER ‘,’ CSV HEADER;

    # ACCESS ANY DATABASE

    psql "postgresql://alkem@alkem:analytics@2022@alkem.postgres.database.azure.com:5432/postgres"

    # SET TO SPECIFIC SCHEMA IN A DB
    SET search_path TO public;

    # Create Table in psql
    CREATE TABLE re_up_1 (
    PartDescription VARCHAR ( 255 ),
    BreakPoint VARCHAR ( 255 ),
    JobCard VARCHAR ( 255 )
    )

    CREATE TABLE re_eo_2 (
    MfgMonthNameYear VARCHAR ( 255 ),
    MfgMonthYear VARCHAR ( 255 ),
    DMart VARCHAR ( 255 ),
    JobCard VARCHAR ( 255 ),
    vehicleChassisNumber VARCHAR ( 255 ),
    FirstYearBucket VARCHAR ( 255 )
    );

    # psql copy comamnd to skip header
    \copy nameage from '/home/user/re/azure_to_postgres/name_age.csv' delimiter ',' CSV HEADER ;

    # Azure Postgres Connection and backup
        # To connect to Azure database
            change the appropriate parameters
            psql --host=mydemoserver.postgres.database.azure.com --port=5432 --username=myadmin@mydemoserver --dbname=postgres
        # To create db and user
            CREATE DATABASE piperrdb;
            # Role --> superuser
            CREATE ROLE piperrdb LOGIN SUPERUSER PASSWORD 'piperrdb';
            GRANT ALL PRIVILEGES ON DATABASE piperrdb TO piperrdb;
        # To restore to that database
	        psql --file=testdb.sql --host=mydemoserver.database.windows.net --port=5432 --username=piperrdb@mydemoserver --dbname=piperrdb
	        psql -h 35.202.189.230 -p 5432 -d superset -U superset -W -f superset.sql

    # airflow postgres connection id
    postgresql+psycopg2://airflow:airflow@34.68.215.72/airflow

    # COPY TABLE FROM ONE SCHEMA TO ANOTHER
        CREATE SCHEMA login_history;
        SHOW search_path;
        SET search_path to login_history;
        create table login_history.user_historic (like public.user_historic including all);
        insert into login_history.user_historic select * from public.user_historic;

    # COPY A POSTGRES TABLE TO CSV
    \COPY user_historic TO 'user_historic_v2.csv' WITH (FORMAT CSV, HEADER);

# COPY FILES FROM KUBERNETES POD TO LOCAL SYSTEM
kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir


# Azure data Downlad command 
az storage blob download \
    --account-name redatalake \
    --container-name 175-warranty-analytics \
    --name poc-data/DMS/Dec_2021_DMS_Jobcard_data.csv \
    --file ~/DMS/Dec_2021_DMS_Jobcard_data.csv \
    --account-key AnWrOm+WVdGEP3POg76d5eIoNaW676NCSiBsDdcmYw3R7Bz5+WkGNL63VQx5Zg4acw2qf4aEkzOfysYdkaFtxg== \
    --auth-mode key

# Merge CSV's
cat dms-cid-2022-08-05.csv > dms-cid-test.csv
sed '1d' dms-cid-2022-08-06.csv >> dms-cid-test.csv

#  to list all ports in use
sudo lsof -i -P -n | grep LISTEN

#  COPY command from local to kubermnetes  pod
kubectl cp /path/to/your_folder name-of-your-pod:/path/to/destination_folder



CREATE SCHEMA public;
# BigQuery command to revert back to yesterday's table
CREATE OR REPLACE TABLE dataset.table_restored
AS 
SELECT *
FROM dataset.table
FOR SYSTEM TIME AS OF 
  TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL -1 DAY)   

#  Using bq command tool
bq load ds.new_tbl ./info.csv ./info_schema.json
bq load ds.new_tbl gs://mybucket/info.csv ./info_schema.json
bq load ds.small gs://mybucket/small.csv name:integer,value:string
bq load ds.small gs://mybucket/small.csv field1,field2,field3

# list all users
ls -la   

# change password incase we forgot password
sudo passwd {username}
https://www.digitalocean.com/community/tutorials/how-to-create-a-new-sudo-enabled-user-on-ubuntu-18-04-quickstart

# to run two commands
(airflow scheduler &) && airflow webserver

# to give root access to 
sudo groupadd user
sudo usermod -aG docker user	
sudo usermod -aG sudo <username>	
# now you can run docker commands without sudo


# install pythpn command
sudo apt install python3.8 -y

# install specific python version in venv
sudo apt update
sudo apt install python3.8-venv -y
python3.8 -m venv {name}

# to find a file in command line
find . -name cred.json


