# Port forwarding Kubernetes
kubectl port-forward piperr-web-6f54547758-6vfhz 8080:8080

# getting inside pod
kubectl exec -i -t piperr-scheduler-76d7c86bd9-pvhtl --container piperr-scheduler -- /bin/bash

# docker socket
sudo chmod +777 /var/run/docker.sock

# deleting port
sudo lsof -i:{nameofport}
sudo kill {pid}

# copies the files fron the container to local file system
docker cp <containerId>:/file/path/within/container /host/path/target

# list of users gcloud
gcloud auth list

# Committing to GIT
1. git clone 'url'
# to switch branch 
git switch 
2. git branch 
3. git diff
4. git status 
5. git add
6. git commit -m "Messages"
7. git push

# To find and replace in all files 
ctrl + shift + f
ctrl + shift + r


# you can also give connections before running dags
Create Pandas equivalent for queries in Trigger cutoff use case data ingestion


Going through the Service Quality DAG to understand the logic used in pandas try to convert it in to a single task using BigQueryOperator
# Merging DAGS
1. Check libraries
2. Merge variables
3. Change to  caps and check for all iknstances
4. Check for DAG name and start date in defined DAGS
5. Import tasks om next DAGS
6. Update Variables
7. Change Email content 
8. Update Dependency

# Git sync access only possible with kubernetes executor

# to display 100 rows in csv file
head -100 filename.csv > cat.csv

# to generate SSH key
ssh-keygen

# docker run in specific port 
docker run -p {target_port}:{container_port} {image_name}

# Running a shell script
1. Create the sh file
2. copy and paste the commands with echo on front
3. check the file through cat command
4. run this ./filename.sh

# Filtering DataFrame Pandas
    # 1.
    stocks_df = pd.DataFrame({
        'Stock': ["Tesla","Moderna Inc","Facebook","Boeing"],
        'Price': [835,112,267,209],
        'Sector':["Technology","Health Technology","Technology","Aircraft"]})
    # 2.
    df1 = df[df['PartNumber']== j & (df['MfgDate']>=mfg_date) & 
    (df['MfgDate']<tpi_bool_value)]

    # 3. to filter through list
    df = df[df['ModelID'].isin(['VDLS46BB', '1800222', 'VWTM42SE'])]

    # 4. Date Conversion
    df['MfgDate'] = pd.to_datetime(df['MfgDate']).dt.date
    x = dt.datetime.strptime(mfg_date, '%Y-%m-%d').date()

# Connecting to VM through SSH
1. Create SSH RSA Asymmetric Public/Private Key
---- ssh-keygen -t rsa -f ~/.ssh/[KEY_FILENAME] -C [USERNAME]
2. Connect to Compute Engine Instance
---- ssh -i PATH_TO_PRIVATE_KEY USERNAME@EXTERNAL_IP
ssh-i ~/.ssh/my_google_cloud_key root0002@34.68.215.72
3. open the specified file in code editor

# imports airflow 1.10.10
import time
import os
import re
from datetime import date, datetime, timedelta
import pandas as pd
from airflow import DAG
from airflow.contrib.hooks.wasb_hook import WasbHook
from airflow.contrib.operators.file_to_gcs import FileToGoogleCloudStorageOperator
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator
from airflow.contrib.operators import gcs_to_bq
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from airflow.contrib.hooks.gcs_hook import GoogleCloudStorageHook
from airflow.utils.dates import days_ago
import smtplib
import mimetypes
import email
import email.mime.application
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication

# Connection GCP / blob
part_list_csv = f'part.csv'
storage_client = storage.Client()
bucket = storage_client.bucket(GCS_BUCKET)
print(storage.Blob(bucket=bucket, name=sales_csv).exists(storage_client))
print(storage.Blob(bucket=bucket, name=part_list_csv).exists(storage_client))
blob1 = bucket.blob(sales_csv)
blob1.download_to_filename("sales.csv")

# Airflow 
# Backfill Command 
airflow backfill FAILURE_PREDICTION_MODEL_DAG_1 -m -s 2022-02-07 -e 2022-06-01"
# Trigger Command 
airflow trigger_dag DMS_SALES_DATA_INGESTION_PIPELINE


uselegacy = false

# Check for a file in azure blob
LOCAL_FILE = f'dms-{today}.csv'
AZURE_BLOB = f'poc-data/DMS/Delta-Data/dms-{today}.csv'
AZURE_CONTAINER = "175-warranty-analytics"
def check_blob():
    az_hook = WasbHook(wasb_conn_id='wasb_default')
    return az_hook.check_for_blob(container_name=AZURE_CONTAINER, blob_name=AZURE_BLOB)

# getting file from a azure blob
LOCAL_FILE = f'dms-{today}.csv'
AZURE_BLOB = f'poc-data/DMS/Delta-Data/dms-{today}.csv'
AZURE_CONTAINER = "175-warranty-analytics"

az_hook = WasbHook(wasb_conn_id='wasb_default')
az_hook.get_file(LOCAL_FILE, container_name=AZURE_CONTAINER, blob_name=AZURE_BLOB)

# Deleting a file in local
def delete_local_files():
  os.remove(LOCAL_FILE)

# For doubt in pandas refer transformation method from (dms JC pipeline)

# Operators in Airflow

gcs = LocalFilesystemToGCSOperator(
    task_id="local_to_gcs",
    src=LOCAL_FILE,
    dst=GCS_FILE_PATH,
    bucket=GCS_BUCKET,
    mime_type='text/csv',
    google_cloud_storage_conn_id=GCP_CONN_ID,
    dag=dag
)

delete = PythonOperator(
    task_id='delete_local_file',
    python_callable=delete_local_files,
    dag=dag
)

gcs_bq = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(
    task_id='gcs_to_bigquery',
    bucket=GCS_BUCKET,
    source_objects=[GCS_FILE_PATH],
    schema_fields=[
        {"name": "JobCard", "type": "STRING", "mode": "NULLABLE"},
        {"name": "PartNumber", "type": "STRING", "mode": "NULLABLE"},
        {"name": "LineAmount", "type": "FLOAT", "mode": "NULLABLE"},
        {"name": "PartDescription", "type": "STRING", "mode": "NULLABLE"},
        {"name": "ItemType", "type": "STRING", "mode": "NULLABLE"}, 
    ],
    destination_project_dataset_table=f'{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}',
    write_disposition='WRITE_APPEND',
    skip_leading_rows=1,
    allow_quoted_newlines=True,
    google_cloud_storage_conn_id=GCP_CONN_ID,
    bigquery_conn_id=GCP_CONN_ID,
    dag=dag
)

email_success = EmailOperator(
        task_id='send_email_on_success',
        to=["rakeshaswath@saturam.com","naveen@saturam.com"],
        subject='DMS JobCard data ingestion pipeline Success',
        html_content=""" <h3>DMS JobCard data ingestion pipeline ingested data successfully</h3> """,
        dag=dag
)

relation_task = BigQueryOperator(
    task_id='relation_tree',
    use_legacy_sql=False,
    sql=sql,
    destination_dataset_table=f'{GCP_PROJECT}:{BQ_DATASET}.{BQ_DESTINATION_TABLE}',
    write_disposition='WRITE_TRUNCATE',
    bigquery_conn_id=GCP_CONN_ID,
    dag=dag,
)

bo = BranchPythonOperator(
    task_id='choose_tasks',
    python_callable=choose_tasks,
    do_xcom_push=False,
    dag=dag
)

# mime_type='text/csv', ASK THIS WITH RAKESH

# converting from test to production dags
1.change destination table file path
2. DAG Date
3. Table Names
4. Send email RE(comment out)


# bash operator shell script
rm /usr/local/airflow/dags/*.csv || echo "hello"
rm rm /usr/local/airflow/dags/*.xls || echo "hello

# variable airflow json
{
"dms_jc_file_status": "false",
"dms_sales_file_status": "false",
"dms_call_centre_file_status": "false",
"dms_rsa_status": "false",
"ppap_file_status": "false",
"rsa_file_status": "false",
"social_media_file_status": "false",
"sap_vehicle_prod_file_status": "false",
"sap_concession_file_status" : "false"
}


# Deployment 


docker exec -u airflow 741b11dfd3e9 pg_dump -Fc airflow > db.sql
sudo -u postgres psql 


# PSQL POSTGRES
CREATE DATABASE naveensdb
\c naveendb
sudo -u postgres psql 



psql -h <REMOTE HOST> -p <REMOTE PORT> -U <DB_USER> <DB_NAME>
psql -h 34.68.215.72 -p 5432 -U superset superset
CREATE DATABASE superset;
CREATE USER superset WITH PASSWORD 'superset';
GRANT ALL PRIVILEGES ON DATABASE "superset" to superset;

psql -h 34.68.215.72 -p 6000 -d postgres -U postgres -W

psql -h 34.68.215.72 -U superset -p 5432 -d superset < /home/root0002/Downloads/superset_bckup.sql

SQLALCHEMY_DATABASE_URI = 'postgresql://superset:superset@34.68.215.72/superset'



